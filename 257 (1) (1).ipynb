{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sO9SSpXQpAyW","outputId":"88c88ee4-e969-4c05-8ba7-030c492e43f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["pip install datasets\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aDNxxVMWU4Ou"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from datasets import load_dataset\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["6c33b681e6c5453fbf8af6cd68501aa1","8c3cf5a87e7c4a21b4cc8d4c3f7372ee","84c394ddfd3749c7ae87247cbab110d7","7d72ba710cf242f8877482069da57ed2","3e3b893b7eb84e30a0280ad845bd30aa","8cee316713ba41b993a587ec8a2af476","e0232b62e5014f79b95d4f3357c4c523"]},"id":"cvdDYxF2U6ab","outputId":"416b87e8-c70c-42a7-c2a4-a61804f4b477"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n","Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n","You are not authenticated with the Hugging Face Hub in this notebook.\n","If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c33b681e6c5453fbf8af6cd68501aa1","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c3cf5a87e7c4a21b4cc8d4c3f7372ee","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84c394ddfd3749c7ae87247cbab110d7","version_major":2,"version_minor":0},"text/plain":["source.spm:   0%|          | 0.00/707k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d72ba710cf242f8877482069da57ed2","version_major":2,"version_minor":0},"text/plain":["target.spm:   0%|          | 0.00/791k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e3b893b7eb84e30a0280ad845bd30aa","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.42M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8cee316713ba41b993a587ec8a2af476","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/310M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0232b62e5014f79b95d4f3357c4c523","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load the tokenizer and model\n","model_name = \"Helsinki-NLP/opus-mt-mul-en\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Il6J6GHlU8Po"},"outputs":[],"source":["# Load your dataset (blog post articles and short headings)\n","train_df = pd.read_csv(\"/content/LABELLED_TRAIN (1).csv\")\n","dev_df = pd.read_csv(\"/content/LABELLED_DEV (1).csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8Ud4NGXLU8xf"},"outputs":[],"source":["# Preprocess the data\n","def preprocess_data(examples):\n","    inputs = examples[\"News Article\"]\n","    targets = examples[\"Caption\"]\n","\n","    # Tokenize inputs and outputs\n","    model_inputs = tokenizer(inputs, max_length=300, truncation=True, padding=\"max_length\")  # Blog post size\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(targets, max_length=45, truncation=True, padding=\"max_length\")    # Short headings\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eQ5g2MCmVEas"},"outputs":[],"source":["\n","# Convert pandas DataFrame to Hugging Face Dataset\n","from datasets import Dataset\n","train_dataset = Dataset.from_pandas(train_df)\n","dev_dataset = Dataset.from_pandas(dev_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["0fdb33bfdfca43a386fc9ee8f250262a","8f8009e51561425588ca13cb8ca32cbc"]},"id":"dUTH_DyXVHrQ","outputId":"7fdb4f88-458e-4d68-ec0d-f03396f2b306"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0fdb33bfdfca43a386fc9ee8f250262a","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f8009e51561425588ca13cb8ca32cbc","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["\n","# Tokenize datasets\n","train_dataset = train_dataset.map(preprocess_data, batched=True)\n","dev_dataset = dev_dataset.map(preprocess_data, batched=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"k4lXjKMRVJ8k","outputId":"b92951b1-f52a-4132-b957-965f1c89de7e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}],"source":["\n","# Define training arguments\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,  # Slightly smaller learning rate for better stability\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    weight_decay=0.01,\n","    save_total_limit=2,\n","    num_train_epochs=3,  # More epochs since data is small\n","    predict_with_generate=True,\n","    logging_dir=\"./logs\",\n","    logging_steps=5,\n","    save_strategy=\"epoch\",\n","    report_to=\"none\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DhEusrIGVMYX","outputId":"c3ffcb62-2e34-4534-a15e-78d3feee2517"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-10-ddd15d291aae>:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n","  trainer = Seq2SeqTrainer(\n"]}],"source":["\n","# Define the trainer\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=dev_dataset,\n","    tokenizer=tokenizer\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":325},"id":"j4AERbQTVOEs","outputId":"f7bf39c5-cab7-4ac1-e164-31833e7c3ad3"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='752' max='752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [752/752 09:51, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.473500</td>\n","      <td>0.558513</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.442200</td>\n","      <td>0.537334</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.433200</td>\n","      <td>0.531078</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.385200</td>\n","      <td>0.530939</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[64171]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n"]},{"data":{"text/plain":["TrainOutput(global_step=752, training_loss=0.48079777168149646, metrics={'train_runtime': 593.4681, 'train_samples_per_second': 20.22, 'train_steps_per_second': 1.267, 'total_flos': 1627121516544000.0, 'train_loss': 0.48079777168149646, 'epoch': 4.0})"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["\n","# Fine-tune the model\n","trainer.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gEvK15cSVQwv","outputId":"004dfef9-bbd5-4967-df74-72834c85476d"},"outputs":[{"data":{"text/plain":["('./fine_tuned_model/tokenizer_config.json',\n"," './fine_tuned_model/special_tokens_map.json',\n"," './fine_tuned_model/vocab.json',\n"," './fine_tuned_model/source.spm',\n"," './fine_tuned_model/target.spm',\n"," './fine_tuned_model/added_tokens.json')"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["\n","# Save the fine-tuned model\n","trainer.save_model(\"./fine_tuned_model\")\n","tokenizer.save_pretrained(\"./fine_tuned_model\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RFdF3SH5U0Pm"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SjZ8hy7Zrhk4","outputId":"27508150-c082-4241-9715-22204262f5c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved as a zip file.\n"]}],"source":["import shutil\n","import os\n","\n","# Define the directory where the model and tokenizer are saved\n","model_dir = \"./fine_tuned_model\"\n","\n","# Create a zip file from the directory\n","shutil.make_archive(model_dir, 'zip', model_dir)\n","\n","# Check if the zip file is created\n","if os.path.exists(f\"{model_dir}.zip\"):\n","    print(\"Model saved as a zip file.\")\n","else:\n","    print(\"Error in saving the model as a zip file.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wpUV959drn9r","outputId":"8bcbd03e-a475-4e12-f751-e69dec7432db"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.67.1)\n","Building wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=c187431239c69c5d8495dfecd01395e61b6a8df3b2e743799f62a01caaa8ab46\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge-score\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.1.2\n"]}],"source":["!pip install rouge-score\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AKU_S-qUdlBL"},"outputs":[],"source":["Print(\"S\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2GLFdQ9VWxj"},"outputs":[],"source":["from rouge_score import rouge_scorer\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","import pandas as pd\n","from datasets import Dataset\n","import torch\n","from tqdm import tqdm  # Import tqdm for progress bar\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qzAMiiurdXvW"},"outputs":[],"source":["\n","# Load the fine-tuned model and tokenizer\n","model_dir = \"./fine_tuned_model\"\n","tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOBVxPT-dPoK"},"outputs":[],"source":["\n","# Load dev dataset (replace with actual dev data path)\n","dev_df = pd.read_csv(\"/content/LABELLED_DEV (1).csv\")  # CSV with 'article' and 'headline' columns\n","\n","# Prepare the dataset for generation\n","def preprocess_data(examples):\n","    inputs = examples[\"News Article\"]\n","    model_inputs = tokenizer(inputs, max_length=300, truncation=True, padding=\"max_length\")\n","    return model_inputs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kLmN4rtwdNoC"},"outputs":[],"source":["\n","# Convert pandas DataFrame to Hugging Face Dataset\n","dev_dataset = Dataset.from_pandas(dev_df)\n","\n","# Tokenize dev dataset\n","dev_dataset = dev_dataset.map(preprocess_data, batched=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7d5nVrOldLzK"},"outputs":[],"source":["# Generate predictions\n","def generate_headlines(model, tokenizer, dataset, batch_size=16):\n","    model.eval()  # Set model to evaluation mode\n","    predictions = []\n","    total_samples = len(dataset)\n","\n","    # Loop with tqdm progress bar to monitor task progress\n","    for i in tqdm(range(0, total_samples, batch_size), desc=\"Generating Headlines\", total=total_samples//batch_size):\n","        inputs = dataset[i:i+batch_size]['input_ids']\n","        attention_mask = dataset[i:i+batch_size]['attention_mask']\n","\n","        # Convert inputs to tensors\n","        input_ids = torch.tensor(inputs).to(model.device)\n","        attention_mask = torch.tensor(attention_mask).to(model.device)\n","\n","        # Generate the headlines\n","        outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=45, num_beams=5, early_stopping=True)\n","\n","        # Decode the outputs back to text\n","        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","        predictions.extend(decoded_preds)\n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RztjZFVxdJz1"},"outputs":[],"source":["# Generate predictions on the dev dataset\n","predicted_headlines = generate_headlines(model, tokenizer, dev_dataset)\n","\n","# Save the predicted headlines and reference headlines to a CSV file\n","output_df = dev_df[['News Article', 'Caption']].copy()\n","output_df['predicted_headline'] = predicted_headlines\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ioP8QOB5dHUF"},"outputs":[],"source":["# Save to CSV\n","output_df.to_csv('/content/generated_headlines.csv', index=False)\n","\n","print(\"Generated headlines saved to 'generated_headlines.csv'\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4c_-TJedE0b"},"outputs":[],"source":["from rouge_score import rouge_scorer\n","import pandas as pd\n","\n","# Load the saved generated headlines CSV file\n","output_df = pd.read_csv('/content/generated_headlines.csv')\n","\n","# Initialize ROUGE scorer\n","rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","# Initialize lists to store ROUGE scores\n","rouge_results = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Cho2_gYdCO8"},"outputs":[],"source":["\n","# Calculate ROUGE scores for each prediction\n","for pred, ref in zip(output_df['predicted_headline'], output_df['Caption']):\n","    scores = rouge_scorer.score(ref, pred)\n","    rouge_results['rouge1'].append(scores['rouge1'].fmeasure)\n","    rouge_results['rouge2'].append(scores['rouge2'].fmeasure)\n","    rouge_results['rougeL'].append(scores['rougeL'].fmeasure)\n","\n","# Compute average ROUGE scores\n","avg_rouge1 = sum(rouge_results['rouge1']) / len(rouge_results['rouge1'])\n","avg_rouge2 = sum(rouge_results['rouge2']) / len(rouge_results['rouge2'])\n","avg_rougeL = sum(rouge_results['rougeL']) / len(rouge_results['rougeL'])\n","\n","# Print average ROUGE scores\n","print(f\"Average ROUGE-1: {avg_rouge1:.4f}\")\n","print(f\"Average ROUGE-2: {avg_rouge2:.4f}\")\n","print(f\"Average ROUGE-L: {avg_rougeL:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5bSyPy6ydCNz"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}